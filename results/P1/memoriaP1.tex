\documentclass[12pt]{article}

%\usepackage{palatino}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}
\usepackage[spanish]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage[x11names, rgb, html]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{geometry}
\usepackage{upgreek}
\usepackage[scaled=.85]{FiraMono}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{listingsutf8}

\hypersetup{
  % hidelinks = true,   % Oculta todos los enlaces.
  colorlinks = true,   % Muestra todos los enlaces, sin bordes alrededor.
  linkcolor={black},     % Color de enlaces genéricos
  citecolor={blue!70!black},   % Color de enlaces de referencias
  urlcolor={blue!70!black}     % Color de enlaces de URL
}

\geometry{left=3cm,right=3cm,top=3cm,bottom=3cm,headheight=1cm,headsep=0.5cm}

\setlength{\parindent}{0pt}

%%% COLORES

\definecolor{50}{HTML}{FFEBEE}
\definecolor{100}{HTML}{FFCDD2}
\definecolor{200}{HTML}{EF9A9A}
\definecolor{300}{HTML}{E57373}
\definecolor{400}{HTML}{EF5350}
\definecolor{500}{HTML}{F44336}
\definecolor{600}{HTML}{E53935}
\definecolor{700}{HTML}{D32F2F}
\definecolor{800}{HTML}{C62828}
\definecolor{900}{HTML}{B71C1C}

%% Colores de Solarized

\definecolor{sbase03}{HTML}{002B36}
\definecolor{sbase02}{HTML}{073642}
\definecolor{sbase01}{HTML}{586E75}
\definecolor{sbase00}{HTML}{657B83}
\definecolor{sbase0}{HTML}{839496}
\definecolor{sbase1}{HTML}{93A1A1}
\definecolor{sbase2}{HTML}{EEE8D5}
\definecolor{sbase3}{HTML}{FDF6E3}
\definecolor{syellow}{HTML}{B58900}
\definecolor{sorange}{HTML}{CB4B16}
\definecolor{sred}{HTML}{DC322F}
\definecolor{smagenta}{HTML}{D33682}
\definecolor{sviolet}{HTML}{6C71C4}
\definecolor{sblue}{HTML}{268BD2}
\definecolor{scyan}{HTML}{2AA198}
\definecolor{sgreen}{HTML}{859900}

%% Colores del documento

\definecolor{text}{RGB}{78,78,78}
\definecolor{accent}{RGB}{129, 26, 24}

%%% LISTINGS

%% Tildes

\lstset{
  inputencoding=utf8/latin1
}

\lstset{
  % How/what to match
   sensitive=false,
  % Border (above and below)
  frame=leftline,
  rulecolor=\color{300},
  framerule=2pt,
  % Line number
  numbers=left,
  % Extra margin on line (align with paragraph)
  xleftmargin=\parindent,
  % Put extra space under caption
  belowcaptionskip=1\baselineskip,
  % Colors
  % backgroundcolor=\color{sbase3},
  basicstyle=\footnotesize\ttfamily\color{sbase00},
  keywordstyle=\color{700},
  commentstyle=\color{300},
  stringstyle=\color{500},
  numberstyle=\color{500},
  %identifierstyle=\color{500},
  % Break long lines into multiple lines?
  breaklines=true,
  % Show a character for spaces?
  showstringspaces=false,
  tabsize=2,
  xleftmargin=0.7em,
}

\renewcommand{\lstlistingname}{Código fuente}% Listing -> Algorithm

\title{Metaheurísticas\\ \Large Aprendizaje de Pesos en Características \\ \large Práctica 1: RELIEF, BL}
\author{Antonio Coín Castro \\ 77191012E \\ antoniocoin@correo.ugr.es \\ Grupo 1 (M 17:30-19:30)}
\date{\today}

\begin{document}
\maketitle

\newpage
\section{Descripción del problema}

En esta práctica y las siguientes consideraremos el marco general de un problema de clasificación. Fijado $n \in \mathbb{N}$, un \textit{clasificador} es cualquier función $c: \mathbb{R}^n \to C$, donde $C$ es un conjunto (finito) de clases prefijadas. Consideramos además un conjunto de \textit{entrenamiento} $T \subseteq \mathbb{R}^n$ de elementos ya clasificados: para cada $t \in T$, denotamos $\Gamma(t) \in C$ a su clase, que es conocida.\\

El problema de clasificación consiste en, dado un conjunto de \textit{prueba} $T'\subseteq \mathbb{R}^n$ no observado previamente, encontrar un clasificador $c$ que maximice el número de clases correctamente clasificadas en $T'$, tras haber sido entrenado sobre los elementos de $T$.\\

Uno de los clasificadores más conocidos y más sencillos es el clasificador $k-$NN, que asigna a cada elemento la clase que más se repite entre sus $k$ vecinos más cercanos. En el caso concreto de esta práctica trabajaremos con el clasificador $1-$NN \textbf{con pesos}: a cada elemento le asignamos la clase de su vecino más cercano, pero ponderamos la distancia en función de un vector de pesos $w \in [0,1]^n$.\\

Para el cálculo de la distancia entre dos elementos de $\mathbb{R}^n$ utilizaremos la \textit{distancia euclídea} ponderada por el ya mencionado vector de pesos:

\[
    d_w(t, s) = \sqrt{\sum_{i=1}^n w_i (s_i - t_i)^2}, \quad t, s \in \mathbb{R}^n
\]
\vspace{.5em}

La idea tras estos pesos es que midan la importancia de cada una de las características que representan las coordenadas de los elementos $n-$dimensionales considerados, asignando más peso en el cálculo de la distancia a aquellas que sean más importantes. El problema de \textbf{aprendizaje de pesos en características} persigue justamente `'aprender'' cuál debe ser el valor de cada peso en una instancia concreta del problema.\\

Para medir la bondad de un clasificador con pesos utilizamos las siguientes métricas:

\begin{itemize}
	\item La \textbf{precisión} (T). Estudiámos cuántos ejemplos del conjunto de prueba se clasifican correctamente, entrenando previamente el clasificador (que utiliza la distancia $d_w$) con el conjunto de entrenamiento.
	\item La \textbf{simplicidad} (R). Un clasificador será más simple si tiene en cuenta un menor número de características. Diremos que una característica $i \in \{1, \dots, n\}$ no se considera en el cálculo de la distancia si el peso asociado $w_i$ es menor que $0.2$. 
\end{itemize}

Así, el problema consiste en encontrar un vector de pesos $w \in [0,1]^n$ que maximice la precisión y la simplicidad, es decir, que maximice lo que llamaremos la \textit{función objetivo}:

\[
    F(w) = \alpha T(w) + (1 - \alpha) R(w).
\]

\newpage

\section{Descripción de la aplicación de los algoritmos}

En esta sección se describen los elementos comunes a todos los algoritmos desarrollados, así como los esquemas de representación de datos de entrada y soluciones. Todo el código se ha desarrollado en \verb|C++11|.
\subsection*{Esquemas de representación}

En primer lugar, los datos de entrada se encuentran en la carpeta \verb|data|. Se trata de tres conjuntos de datos \textbf{ya normalizados} en formato \verb|csv|, donde cada fila representa un ejemplo con los valores de sus características separados por '\verb|;|' y el último elemento de la fila es su clase.\\

Para representar los datos en el programa se emplea una estructura \verb|Example| que recoge toda la información necesaria: un \verb|vector<double>| con los valores de cada una de las \verb|n| características del ejemplo concreto, así como un \verb|string| que representa su clase o categoría.

\begin{verbatim}
struct Example {
    vector<double> traits;
    string category;
    int n;
}
\end{verbatim}

Cada conjunto de datos se representa entonces por un \verb|vector<Example>|, y se emplea la función \verb|read_csv| para rellenar el vector, que va leyendo los archivos línea a línea.\\ 

Además, como será necesario hacer particiones de cada conjunto de datos para implementar la técnica de \textit{$K$-fold cross validation}, se proporciona la función \verb|make_partitions| que se encarga de repartir los elementos entre los $K$ conjuntos considerados, respetando la proporción original de clases. La forma de hacer esto es simplemente ir poniendo cada clase de forma cíclica en las particiones, primando el reparto equitativo de clases al reparto equitativo de elementos.\\

Por su parte, la solución es un \verb|vector<double>| del mismo tamaño que el número de categorías consideradas en cada caso. La componente $i-$ésima del vector representa el peso otorgado a la característica $i-$ésima de cada ejemplo del problema.

\subsection*{Operadores comunes}

Todos los algoritmos hacen uso del cálculo de la distancia. Como dijimos, para este cálculo se emplea la distancia euclídea, eventualmente ponderada mediante un vector de pesos. En el caso de que la distancia deseada sea la estándar, se asume que los pesos valen siempre $1$ (en la implementación realmente hay dos funciones separadas, una con pesos y otra sin pesos).\\

\newpage 

\begin{algorithm}[h]
\begin{algorithmic}

\Function{distance\_sq\_weights}{e1, e2, w}
     
     \State distance = $0$
     \For{i $:=$ $0$ to $n-1$}  \Comment{n es el número de características}
         \If {w[i] $\geq$ 0.2}
            \State distance += w[i] * (e2[i] - e1[i]) * (e2[i] - e1[i])
\EndFunction
\end{algorithmic}
\end{algorithm}

Cabe destacar que en realidad estamos calculando al distancia euclídea al cuadrado, pues solo vamos a utilizarla para comparar. Como la función $f(x)=\sqrt{x}$ es creciente para $x\geq 0$ no hay problema en que hagamos esto, pues se mantiene el orden. De esta forma ahorramos tiempo de cálculo, pues esta función va a ser llamada muchas veces a lo largo del programa.\\

También tenemos la función \verb|classifier_1nn_weights|, que clasifica un ejemplo basándose en la técnica del vecino más cercano. Debemos pasarle también el conjunto de entrenamiento con los ejemplos ya clasificados, y el vector de pesos. De nuevo, si queremos que el clasificador no tenga en cuenta los pesos podemos asumir que son todos $1$, aunque en realidad hay dos funciones separadas.

\begin{algorithm}[h]
\begin{algorithmic}

\Function{classifier\_1nn\_weights}{e, training, self, w}
     \State selected = $0$
     \State dmin = $\infty$
     \For{i $:=$ $0$ to $n-1$}  \Comment{n es el número de ejemplos de entrenamiento}
         \If {i $\neq$ self}
            \State dist = distance\_sq\_weights(e, training[i], w)
            \If {dist $<$ dmin}
                \State dmin = dist
                \State selected = i

   \Return training[selected].category
\EndFunction

\end{algorithmic}
\end{algorithm}

\subsection*{Función objetivo}

La función objetivo que queremos maximizar se implementa tal y como se dijo en la descripción del problema, donde el valor $\alpha$ prefijado es de 0.5, dando la misma importancia a la precisión y a la simplicidad.

\begin{verbatim}
objective(class_rate, red_rate) {
  return alpha * class_rate + (1.0 - alpha) * red_rate;
}
\end{verbatim}

Para calcular la tasa de clasificación y de reducción utilizamos otras funciones también muy sencillas. La primera mide el porcentaje de acierto sobre un vector de elementos que el clasificador ha clasificado, y cuya clase real conocemos. La segunda simplemente contabiliza qué porcentaje de los pesos son menores que $0.2$.

\begin{algorithm}[H]
\begin{algorithmic}

\Function{class\_rate}{classified, test}
     
     \State correct = $0$
     \For{i $:=$ $0$ to $n-1$}  \Comment{n es el número de ejemplos clasificados}
         \If {classified[i] $==$ test[i].category}
            \State correct++

  \Return $100.0$ * correct / n
  \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}

\Function{red\_rate}{w}
     
     \State discarded = $0$
     \For{i $:=$ $0$ to $n-1$}  \Comment{n es el tamaño del vector de pesos}
         \If {w[i] $<$ $0.2$}
            \State discarded++

  \Return $100.0$ * discarded / n
  \EndFunction
\end{algorithmic}
\end{algorithm}

\newpage
\section{Descripción de los algoritmos considerados}
En esta sección se describen los dos algoritmos implementados en esta práctica para el problema del APC. En ambos lo que se pretende es rellenar un vector de pesos para maximizar la función objetivo.

\subsection*{Algoritmo \textit{greedy} RELIEF}

Este es un algoritmo voraz muy sencillo, que servirá como caso base para comparar las diferentes metaheurísticas desarrolladas. Se trata de buscar, para cada ejemplo, su amigo (misma clase) y su enemigo (distinta clase) más cercano. Después, componente a componente se suma al vector de pesos la distancia a su enemigo, y se resta la distancia a su amigo.\\

En este proceso es posible que los pesos se salgan del intervalo $[0,1]$, por lo que al finalizar es necesario normalizarlos. Además, no debemos olvidar que el algoritmo comienza con el vector de pesos relleno de $0$s. Disponemos para ello de una función \verb|init_vector| que se encarga de darle un valor inicial de $0$ a todas las componentes del vector de pesos.\\

Separamos la función que se encarga de buscar los amigos y enemigos más cercanos, teniendo en cuenta que el amigo más cercano no puede ser el propio ejemplo.

\begin{algorithm}[h]
\begin{algorithmic}

\Function{nearest\_example}{training, e, self}
     \State dmin\_friend = $\infty$
     \State dmin\_enemy = $\infty$
     \For{i $:=$ $0$ to $n-1$}  \Comment{n es el número de ejemplos de entrenamiento}
         \If {i $\neq$ self}
            \State dist = distance\_sq(e, training[i])
            \If {training[i].category $!=$ e.category \textit{and} dist < dmin\_enemy}
                  \State n\_enemy = i
                  \State dmin\_enemy = dist
            \ElsIf {training[i].category == e.category \textit{and} dist < dmin\_friend)}
                  \State n\_friend = i
                  \State dmin\_friend = dist

   \Return n\_enemy, n\_friend
\EndFunction

\end{algorithmic}
\end{algorithm}

El algoritmo RELIEF se detalla ya en la página siguiente.
\newpage

\begin{algorithm}[h!]
\begin{algorithmic}

\Function{relief}{training}
     \State w = init\_vector()
     \For{i $:=$ $0$ to $n-1$}  \Comment{n es el número de ejemplos de entrenamiento}
         \State n\_enemy, n\_friend = nearest\_example(training, training[i], i)
         \For{j $:=$ $0$ to $m - 1$}  \Comment {m es el tamaño del vector de pesos}
            \State w[j] = w[j] + $|$training[i].traits[j] - training[n\_enemy].traits[j]$|$
            \State \hspace{2.5em} $-$ $|$training[i].traits[j] - training[n\_friend].traits[j]$|$
          
         \EndFor
      \EndFor
      \State max = max(w)
      \For {j $:=$ $0$ to $m-1$}  \Comment{normalizamos los pesos}
        \If {w[j] $<$ 0}
            \State w[j] = 0
        \Else
            \State w[j] = w[j] / max
        \EndIf
      
\hspace{-.7em} \Return w
\EndFunction

\end{algorithmic}
\end{algorithm}


\subsection*{Algoritmo de búsqueda local}

Empleamos la técnica de búsqueda local del \textbf{primer mejor} para rellenar el vector de pesos. La idea es mutar en cada iteración una componente aleatoria y \textbf{distinta} del vector de pesos, sumándole un valor extraído de una normal de media $0$ y desviación típica $\sigma = 0.3$. Si tras esta mutación se mejora la función objetivo, nos quedamos con este nuevo vector, y si no lo desechamos. Si algún peso se sale del intervalo $[0,1]$ tras la mutación, directamente lo truncamos a $0$ ó a $1$.\\

Para la generación de la solución inicial sobre la que iterar, consideramos valores extraídos de una distribución uniforme $\mathcal U(0,1)$, que obtenemos gracias al tipo (de la librería \verb|<random>|) \verb|uniform_real_distribution<double>|. A la hora de escoger qué componente vamos a mutar, tenemos un vector de índices del mismo tamaño que el vector de pesos, que barajamos de forma aleatoria y recorremos secuencialmente. Si llegamos al final, volvemos a barajarlo para seguir generando nuevas soluciones.

\begin{verbatim}
// Initialize index vector and solution
for (int i = 0; i < n; i++) {
  index.push_back(i);
  w[i] = uniform_real(gen);
}
shuffle(index.begin(), index.end(), gen);
\end{verbatim}

Para escoger el valor con el que se muta cada componente utilizamos esta vez el tipo predefinido \verb|normal_distribution<double>|. Para determinar si una mutación mejora, utilizamos como métrica el valor de la función objetivo, tomando la tasa de clasificación sobre el propio conjunto de entrenamiento (\textit{leave-one-out}).

\newpage

En primer lugar, separamos la comprobación de mejora en la función objetivo para más claridad.

\begin{algorithm}[ht]
\begin{algorithmic}

\Function{evaluate}{training, classified, w}
\For {i $:=$ $0$ to $n-1$}  \Comment n es el número de ejemplos de entrenamiento
    \State classified.push\_back(classifier\_1nn\_weights(training[i], training, i, w))
\EndFor

  \State o = objective(class\_rate(classified, training), red\_rate(w))
  \State classified.clear()
  
\hspace{-.7em} \Return o

\end{algorithmic}
\end{algorithm}

Mostramos ahora el procedimiento de la búsqueda local.

\begin{algorithm}[h!]
\begin{algorithmic}

\Function{local\_search}{training}

\State w, index = initialize() \Comment ejecuta el código de inicialización mostrado anteriormente

\State best\_objective = evaluate(training, classified, w)

  \While {iter $<$ MAX\_ITER \textit{and} neighbour $<$ n * MAX\_NEIGHBOUR\_PER\_TRAIT}
    \State comp = index[iter $\%$ n]
    \State w\_mut = w
    \State w\_mut[comp] += normal(gen)
    
    \If {w\_mut[comp] $>$ $1$} \State w\_mut[comp] = $1$
    \ElsIf {w\_mut[comp] $<$ $0$} \State w\_mut[comp] = $0$
    \EndIf
    \State current\_objective = evaluate(training, classified, w)
    \State iter++

    \If {current\_objective $>$ best\_objective}
      \State mut++
      \State neighbour = $0$
      \State w = w\_mut
      \State best\_objective = current\_objective
      \State improvement = true

    \Else 
      \State neighbour++
    \EndIf

    \If {iter $\%$ $n == 0$ \textit{or} improvement}
      \State shuffle(index.begin(), index.end(), gen)
      \State improvement = false
    \EndIf
      
\hspace{-.7em} \Return w
\EndFunction

\end{algorithmic}
\end{algorithm}

Notamos que detenemos la búsqueda cuando llegamos al máximo de iteraciones, o cuando generamos un número de vecinos (dependiente del número de características) sin mejorar la función objetivo.

\newpage

\section{Procedimiento considerado para desarrollar la práctica}

Todo el código de la práctica se ha desarrollado en C++ siguiendo el estándar 2011. Se utiliza la biblioteca \verb|std| y otras bibliotecas auxiliares, pero no se ha hecho uso de ningún \textit{framework} de metaheurísticas.\\

Para todos los procedimientos que implican aleatoreidad se utiliza un generador de números aleatorios común (llamado \verb|gen|), inicializado con una semilla concreta. La semilla por defecto es $20$, aunque se puede especificar otra mediante línea de comandos. La evaluación de los tres algoritmos considerados ($1-$NN, RELIEF y búsqueda local) se realiza mediante la función \verb|run|.\\

Se proporciona un makefile para compilar los archivos y generar un ejecutable, mediante la orden \verb|make|. A la hora de ejecutarlo hay dos opciones:

\begin{itemize}
    \item Pasarle como parámetro una semilla para el generador aleatorio, y a continuación una lista de archivos sobre los que ejecutar los algoritmos (ruta relativa).
    \item Ejecutarlo sin argumentos. En este caso, utiliza la semilla por defecto y ejecuta los algoritmos sobre los tres conjuntos de datos de la carpeta \verb|DATA|.
\end{itemize}

El código está disponible en la carpeta \verb|FUENTES|, y consta de los siguientes módulos:

\begin{itemize}
	\item \verb|p1.cpp| Contiene la implementación de los algoritmos, y las funciones necesarias para ejecutarlos.
	\item \verb|timer| Módulo para medir tiempos en UNIX.
	\item \verb|util| Se trata de funciones auxiliares para el preprocesamiento de los archivos de datos, el cálculo de la distancia, etc.
\end{itemize}

Al compilar se genera un único ejecutable en la carpeta \verb|BIN| de nombre \verb|p1|.

\newpage

\section{Resultados}

\subsection*{Descripción de los casos del problema}

Se consideran tres conjuntos de datos sobre los que ejecutar los algoritmos:

\begin{itemize}
	\item \textbf{Colposcopy.} La colposcopia es un procedimiento ginecológico que
consiste en la exploración del cuello uterino. Consta de 287 ejemplos, 62 atributos reales y dos clases: positivo o negativo.
    \item \textbf{Ionosphere.} Datos de radar recogidos por un sistema en Goose Bay, Labrador. Consta de 351 ejemplos, 34 atributos y dos clases: retornos buenos (g) y malos (b).
    \item \textbf{Texture.} El objetivo de este conjunto de datos es distinguir entre 11
texturas diferentes. Consta de 550 ejemplos, 40 atributos y 11 clases (tipos de textura).
\end{itemize}

\subsection*{Resultados obtenidos}

A continuación se muestran las tablas de unos resultados obtenidos para cada uno de los algoritmos. El orden de las columnas es siempre el mismo: primero \textit{colposcopy}, después \textit{ionosphere}, y por último \textit{texture}.\\

Para cada conjunto de datos se muestra una tabla con cada una de las 5 ejecuciones realizadas, de acuerdo a la técnica $5-$\textit{fold cross validation}. En cada una de ellas se muestran los valores de la tasa de clasificación (Clas), tasa de reducción (Red), función objetivo (Agr) y tiempo de ejecución (T) \textbf{en milisegundos}. Además, se muestra finalmente una tabla global con los resultados medios de cada conjunto de datos para todos los algoritmos. Esta información también se recoge en la última fila de las tablas de cada algoritmo.\\

\textbf{Clasificador $\boldsymbol{1-}$NN sin pesos}

\begin{table}[h]
\begin{tabular}{ccccc|cccc|cccc}
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
1           & 72.88         & 0            & 36.44        & 0.88       & 83.09        & 0            & 41.54        & 2.50      & 94.54         & 0            & 47.27        & 1.93       \\
2           & 75.43            & 0            & 37.71          & 0.81       & 84.28         & 0            & 42.14        & 0.92       & 88.18         & 0            & 44.09        & 1.90       \\
3           & 78.94         & 0            & 39.47        & 0.80       & 94.28         & 0            & 47.14        & 0.62       & 90.90         & 0            & 45.45        & 1.91       \\
4           & 82.45        & 0            & 41.22        & 0.80       & 85.71        & 0            & 42.85        & 0.61       & 96.36         & 0            & 48.18        & 1.91       \\
5           & 68.42         & 0            & 34.21        & 0.80       & 88.57         & 0            & 44.28        & 0.61       & 90.00         & 0            & 45.00        & 1.95       \\ \hline
$\bar{x}$           & 75.62         & 0            & 37.81        & 0.82       & 87.19         & 0            & 43.59       & 1.05       & 92.00        & 0            & 46.00        & 1.92
\end{tabular}
\end{table}

\newpage

\textbf{Algoritmo RELIEF}

\begin{table}[h]
\begin{tabular}{ccccc|cccc|cccc}
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
1           & 71.18         & 20.96        & 46.07        & 4.65      & 84.50         & 2.94        & 43.72        & 13.55      & 94.54         & 2.50           & 48.52        & 10.30      \\
2           & 77.19         & 24.19         & 50.69        & 4.19      & 84.28         & 2.94        & 43.61        & 3.82      & 89.09         & 5.00          & 47.04        & 10.48      \\
3           & 71.92        & 30.64         & 51.28        & 4.52      & 94.28         & 2.94        & 48.61        & 3.67      & 92.72         & 5.00          & 48.86        & 10.49      \\
4           & 77.19         & 72.58         & 74.88        & 4.05      & 88.57         & 2.94        & 45.75        & 3.61      & 97.27         & 17.5          & 57.38        & 9.97     \\
5           & 63.15            & 32.25         & 47.70        & 4.17      & 90.00         & 2.94        & 46.47        & 3.60      & 92.72         & 5.00            & 48.86        & 11.70      \\ \hline
$\bar{x}$           & 72.13         & 36.12         & 54.13        & 4.32      & 88.33         & 2.94        & 45.63        & 5.65      & 93.27         & 7.00          & 50.13        & 10.59
\end{tabular}
\end{table}

\textbf{Algoritmo de búsqueda local}

\begin{table}[ht]
\begin{tabular}{ccccc|cccc|cccc}
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
1           & 71.18        & 83.87        & 77.52        & 10524    & 87.32         & 82.35        & 84.83        & 3634    & 91.81         & 82.50           & 87.15        & 19733     \\
2           & 68.42            & 87.09        & 77.75        & 16717    & 85.71         & 85.29        & 85.50        & 6378    & 89.09         & 82.50         & 85.79        & 21246    \\
3           & 80.70        & 80.64        & 80.67        & 17315    & 88.57         & 85.29        & 86.93        & 7142    & 90.00         & 82.50         & 86.25        & 17312    \\
4           & 75.43         & 79.03        & 77.23        & 13554    & 84.28         & 88.23        & 86.26        & 5094    & 90.90         & 82.50           & 86.70        & 26170    \\
5           & 57.89            & 85.48        & 71.68        & 15536    & 90.00         & 91.17        & 90.58      & 7746    & 88.18         & 87.50         & 87.84        & 26074    \\ \hline
$\bar{x}$           & 70.72         & 83.22        & 76.97        & 14729    & 87.17         & 86.47        & 86.82        & 5999    & 90.00         & 83.50         & 86.75        & 22107
\end{tabular}
\end{table}


\textbf{Resumen global}

\begin{table}[ht]
\begin{tabular}{ccccc|cccc|cccc}
\textbf{Nº} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} & \textbf{Clas} & \textbf{Red} & \textbf{Agr} & \textbf{T} \\ \hline
1-NN           & 75.62         & 0            & 37.81        & 0.82       & 87.19         & 0            & 43.59       & 1.05       & 92.00        & 0            & 46.00        & 1.92     \\
RELIEF           & 72.13         & 36.12         & 54.13        & 4.32      & 88.33         & 2.94        & 45.63        & 5.65      & 93.27         & 7.00          & 50.13        & 10.59    \\
BL           & 70.72         & 83.22        & 76.97        & 14729    & 87.17         & 86.47        & 86.82        & 5999    & 90.00         & 83.50         & 86.75        & 22107    \\
\end{tabular}
\end{table}

\subsection*{Análisis de resultados}

En primer lugar, observamos que las tasas de clasificación obtenidas mediante el clasificador $1-$NN sin pesos, aunque pueden llegar a ser altas según el \textit{dataset}, no contribuyen en exceso al agregado total. Esto era de esperar, pues en este clasificador la simplicidad es siempre $0$, la mínima posible.\\

Tomando como ejemplo el conjunto de datos \textbf{texture}, vemos que tiene una tasa de clasificación media de $92\%$ en este ejemplo, por lo que podemos concluir que las características medidas están bien elegidas. Sin embargo, el conjunto \textbf{colposcopy} tiene una tasa de clasificación media de solo $75\%$, lo que nos hace pensar que podríamos mejorar seleccionando con más cuidado las características a medir.\\

En cuanto al algoritmo RELIEF, al tratarse de un algoritmo \textit{greedy} es posible que no proporcione siempre la mejor solución. Al permitir que la tasa de reducción no sea $0$ estamos aumentando la simplicidad, y por tanto potencialmente aumentando el valor de la función objetivo. Vemos que efectivamente esto es lo que ocurre, pues consistentemente en todos los conjuntos de datos mejoramos dicho valor con respecto al clasificador sin pesos. Además, el tiempo de ejecución de este algoritmo sigue siendo despreciable, por lo que constituye un buen punto de partida para la comparación con otras metaheurísticas.\\

Por último, el algoritmo de búsqueda local es el más costoso en tiempo, llegando a tardar del orden de 1000 veces más por partición que los anteriores. Sin embargo, este algoritmo consigue tasas de reducción muy altas, es decir, aumenta mucho la simplicidad del clasificador. Al ser un algoritmo de búsqueda local típicamente caemos en óptimos locales, por lo que no conseguimos la solución óptima. Sin embargo, en muchas ocasiones llegamos a una solución \textit{suficientemente buena}, como demuestran los resultados obtenidos.\\

En este último algoritmo conseguimos aumentar considerablemente el valor de la función objetivo en todos los casos, rondando el 80$\%$, a costa de un tiempo de ejecución mayor (aunque todavía viable). Aún queda ver cómo se comporta este algoritmo relativamente sencillos con otros que implementemos en el futuro.\\

Por último, cabe destacar que en cuanto a la tasa de clasificación, el algoritmo RELIEF es competitivo con el de la búsqueda local, llegando a ser mejor que este último en varias ocasiones. En cualquier caso, el algoritmo voraz no consigue llegar (ni se acerca) a las tasas de reducción que consigue la búsqueda local, que converge rápidamente a un óptimo local, aunque pueda tener períodos de estancamiento.

\end{document}

